---
type: post
date: ~2017.10.7..05.32.55..da29
title: Ford and Unicode: Table Generation in the Build System
author: ~littel-ponnys
navsort: bump
navuptwo: true
comments: reverse
---

Take the current `++cuss`, which is the current name of the gate which uppercases text:

```
> (cuss "ex√°mple")
"EX√°MPLE"
```

It doesn't deal with characters outside of 7-bit ASCII. This isn't ideal for software which aspires to deal with the Real Internet! But how do you deal with all the different characters in the Unicode Standard?

You parse [this file](http://www.unicode.org/Public/9.0.0/ucd/UnicodeData.txt).

Most language runtimes handles this by having a script which consumes various unicode definition files and spits out statically generated tables which get compiled into their support library. [The golang implementation](https://golang.org/src/unicode/maketables.go), by way of example.

Static code generators don't feel very urbit-ish though. If only we had a generalized build system which could perform arbitrary transformations...

`%ford` can read files, perform mark transformations, and run arbitrary code as a build process. If you put the UnicodeData.txt file in your urbit, write a mark to convert text to a usable representation, and then perform computation on that data and stick it in as part of the subject.

Let's take a look at the [main part](https://github.com/eglaysher/arvo/blob/1f16f860b7f3d83b108c2f1b9586c25c5a57527e/gen/capitalize.hoon) of my proof of concept patch:

```
::  a map of characters to their uppercase representation
/=  uppers
  /;  |=  a/(list line:unicode-data)
      =|  ret/(map @c @c)
      |-
      ^-  (map @c @c)
      ?~  a
        ret
      ?~  up.i.a
        $(a t.a)
      $(a t.a, ret (~(put by ret) code.i.a u.up.i.a))
  /:  /===/lib/unicode-data  /&unicode-data&/txt/
```

`/=` is a horn (a ford rune) which wraps its second argument in a face. `/;` runs an arbitrary gate on its second argument. And `/:` evaluates a path and runs a mark pipeline. In the above code, we take the text file, pass it through the unicode-data mark, and take the result of that for a post-processing function which creates the mapping necessary for runtime lookup.

This completely skips static code generation; the data ends up processed in the subject of our generator. The first time you run `+capitalize`, it takes a few moments because `%ford` is doing all this processing for you, but further  (assuming you don't change UnicodeData.txt) 

```
> +capitalize "ex√°mple"
"EX√ÅMPLE"
> +capitalize "·∏ø·ª∑ ·∏π√Ø≈ß≈ß·∏Ω·∏ù ∆•·πì·πâ√ø"
"·∏æ·ª∂ ·∏∏√è≈¶≈¶·∏º·∏ú ∆§·πí·πà≈∏"
```

(ü§òü§ò What do you mean it's not metal?! ü§òü§ò)

So this prototype works and you can [patch it in](https://github.com/eglaysher/arvo/commit/1f16f860b7f3d83b108c2f1b9586c25c5a57527e) if you want to play with it. I have a few remaining questions though before I work on building a full unicode aware library:

- Should I be using one of these mysterious %/ren files instead of building a mark to parse UnicodeData.txt?

- The same code excerpted above doesn't work when moved from a generator to a library. The `uppers` doesn't show up in the subject when the same horn construct is moved to a library. Is this supposed to work?
